{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Geron10_Intro to ANN with Keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RocioLiu/ML_Resources/blob/master/Geron10_Intro_to_ANN_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbR0FGfNytMG",
        "colab_type": "text"
      },
      "source": [
        "## Implementing MLPs with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JT7xvdAx4Px",
        "colab_type": "text"
      },
      "source": [
        "### Building an Image Classifier Using the Sequential API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ4keHlc94mW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzSxTkxu953A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "72cec08c-8fb9-45d8-988e-0ae2d349044a"
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFf1MU7W99wM",
        "colab_type": "code",
        "outputId": "6a4182d7-4288-4047-b375-87090a414910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train_full.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp03I5hG-CAF",
        "colab_type": "code",
        "outputId": "8f65261b-7248-4136-988a-3888a00541b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train_full.dtype"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('uint8')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcuPc7Bt-GcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeVMwrFY-_Xc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPhWNJ31_CIC",
        "colab_type": "code",
        "outputId": "4d33b7ac-e7b8-4382-ecb4-aa84a8058b0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class_names[y_train[0]]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Coat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkQSnO_HyJCi",
        "colab_type": "text"
      },
      "source": [
        "#### Creating the Model Using the Sequential API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqaW2c0G_cyC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "4c352cd9-78ba-434a-c7de-f97750b0987f"
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
        "model.add(keras.layers.Dense(300, activation = 'relu'))\n",
        "model.add(keras.layers.Dense(100, activation = 'relu'))\n",
        "model.add(keras.layers.Dense(10, activation = 'softmax'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0710 08:47:57.492129 139636251805568 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RblqPDlcFObh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28,28]),\n",
        "    keras.layers.Dense(300, activation = 'relu'),\n",
        "    keras.layers.Dense(100, activation = 'relu'),\n",
        "    keras.layers.Dense(10, activation = 'softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKxRbzsSHc09",
        "colab_type": "code",
        "outputId": "3cab3dec-afb1-4d9c-e67f-562eae0d4565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.layers\n",
        "model.layers[1].name\n",
        "#model.get_layer('dense_3').name"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dense_3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7-EJnkMKZ23",
        "colab_type": "code",
        "outputId": "7602474c-bf03-401a-c407-0c61d1610584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "weights, biases = model.layers[1].get_weights()\n",
        "weights"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01503728, -0.05699985,  0.01687996, ..., -0.05489433,\n",
              "         0.01371884,  0.04520964],\n",
              "       [ 0.04580186,  0.00427995,  0.06365997, ..., -0.04532593,\n",
              "         0.01539738,  0.06797412],\n",
              "       [ 0.06407946,  0.00896972, -0.06395225, ..., -0.03237554,\n",
              "        -0.00124098,  0.02909163],\n",
              "       ...,\n",
              "       [-0.06740418,  0.04015753,  0.03372011, ..., -0.0542163 ,\n",
              "        -0.02707728,  0.029973  ],\n",
              "       [ 0.00637474,  0.06874329,  0.01942719, ..., -0.04886399,\n",
              "         0.00256976, -0.02806403],\n",
              "       [ 0.03410461,  0.04210812, -0.04232227, ...,  0.06503583,\n",
              "        -0.02423177, -0.04210303]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF5SldJHLB4t",
        "colab_type": "code",
        "outputId": "38056623-f442-4007-c0a5-298603383528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "weights.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCUu_BACpURE",
        "colab_type": "code",
        "outputId": "6f07a52e-08df-48f7-a41a-4ee9ddef8fc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "biases[:10]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH52rpzeLR38",
        "colab_type": "code",
        "outputId": "913dc3d2-0f40-463a-ce76-9d5c1edf00a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "biases.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxPaP3uIyQpL",
        "colab_type": "text"
      },
      "source": [
        "#### Compiling the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWIBR66dQFgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wuZDVkEyUgS",
        "colab_type": "text"
      },
      "source": [
        "#### Training and Evaluating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ3JV4PULcaq",
        "colab_type": "code",
        "outputId": "7313d2f9-5403-4947-91bf-67a5505b34b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_fit = model.fit(X_train, y_train, epochs = 30, validation_data = (X_valid, y_valid))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 55000 samples, validate on 5000 samples\n",
            "Epoch 1/30\n",
            "55000/55000 [==============================] - 6s 115us/sample - loss: 0.6995 - acc: 0.7705 - val_loss: 0.5347 - val_acc: 0.8194\n",
            "Epoch 2/30\n",
            "55000/55000 [==============================] - 6s 109us/sample - loss: 0.4842 - acc: 0.8319 - val_loss: 0.4437 - val_acc: 0.8496\n",
            "Epoch 3/30\n",
            "55000/55000 [==============================] - 6s 106us/sample - loss: 0.4388 - acc: 0.8469 - val_loss: 0.4087 - val_acc: 0.8588\n",
            "Epoch 4/30\n",
            "55000/55000 [==============================] - 6s 108us/sample - loss: 0.4124 - acc: 0.8544 - val_loss: 0.3970 - val_acc: 0.8644\n",
            "Epoch 5/30\n",
            "55000/55000 [==============================] - 6s 107us/sample - loss: 0.3928 - acc: 0.8622 - val_loss: 0.3873 - val_acc: 0.8644\n",
            "Epoch 6/30\n",
            "55000/55000 [==============================] - 6s 105us/sample - loss: 0.3770 - acc: 0.8678 - val_loss: 0.3742 - val_acc: 0.8680\n",
            "Epoch 7/30\n",
            "55000/55000 [==============================] - 6s 109us/sample - loss: 0.3642 - acc: 0.8701 - val_loss: 0.3559 - val_acc: 0.8772\n",
            "Epoch 8/30\n",
            "55000/55000 [==============================] - 6s 110us/sample - loss: 0.3523 - acc: 0.8753 - val_loss: 0.3553 - val_acc: 0.8764\n",
            "Epoch 9/30\n",
            "55000/55000 [==============================] - 6s 113us/sample - loss: 0.3425 - acc: 0.8777 - val_loss: 0.3654 - val_acc: 0.8676\n",
            "Epoch 10/30\n",
            "55000/55000 [==============================] - 6s 112us/sample - loss: 0.3328 - acc: 0.8816 - val_loss: 0.3448 - val_acc: 0.8786\n",
            "Epoch 11/30\n",
            "55000/55000 [==============================] - 6s 114us/sample - loss: 0.3249 - acc: 0.8833 - val_loss: 0.3399 - val_acc: 0.8818\n",
            "Epoch 12/30\n",
            "55000/55000 [==============================] - 6s 111us/sample - loss: 0.3169 - acc: 0.8866 - val_loss: 0.3497 - val_acc: 0.8766\n",
            "Epoch 13/30\n",
            "55000/55000 [==============================] - 6s 115us/sample - loss: 0.3098 - acc: 0.8887 - val_loss: 0.3268 - val_acc: 0.8808\n",
            "Epoch 14/30\n",
            "55000/55000 [==============================] - 6s 114us/sample - loss: 0.3030 - acc: 0.8914 - val_loss: 0.3416 - val_acc: 0.8776\n",
            "Epoch 15/30\n",
            "55000/55000 [==============================] - 6s 113us/sample - loss: 0.2967 - acc: 0.8922 - val_loss: 0.3211 - val_acc: 0.8858\n",
            "Epoch 16/30\n",
            "55000/55000 [==============================] - 6s 111us/sample - loss: 0.2901 - acc: 0.8947 - val_loss: 0.3188 - val_acc: 0.8886\n",
            "Epoch 17/30\n",
            "55000/55000 [==============================] - 6s 114us/sample - loss: 0.2852 - acc: 0.8965 - val_loss: 0.3123 - val_acc: 0.8902\n",
            "Epoch 18/30\n",
            "55000/55000 [==============================] - 6s 109us/sample - loss: 0.2794 - acc: 0.8996 - val_loss: 0.3179 - val_acc: 0.8892\n",
            "Epoch 19/30\n",
            "55000/55000 [==============================] - 7s 124us/sample - loss: 0.2750 - acc: 0.9010 - val_loss: 0.3257 - val_acc: 0.8828\n",
            "Epoch 20/30\n",
            "55000/55000 [==============================] - 6s 108us/sample - loss: 0.2700 - acc: 0.9019 - val_loss: 0.3088 - val_acc: 0.8920\n",
            "Epoch 21/30\n",
            "55000/55000 [==============================] - 6s 113us/sample - loss: 0.2652 - acc: 0.9040 - val_loss: 0.3031 - val_acc: 0.8944\n",
            "Epoch 22/30\n",
            "55000/55000 [==============================] - 6s 112us/sample - loss: 0.2596 - acc: 0.9055 - val_loss: 0.3145 - val_acc: 0.8852\n",
            "Epoch 23/30\n",
            "55000/55000 [==============================] - 6s 110us/sample - loss: 0.2559 - acc: 0.9079 - val_loss: 0.3038 - val_acc: 0.8904\n",
            "Epoch 24/30\n",
            "55000/55000 [==============================] - 6s 110us/sample - loss: 0.2512 - acc: 0.9089 - val_loss: 0.2962 - val_acc: 0.8962\n",
            "Epoch 25/30\n",
            "55000/55000 [==============================] - 6s 110us/sample - loss: 0.2480 - acc: 0.9105 - val_loss: 0.3039 - val_acc: 0.8914\n",
            "Epoch 26/30\n",
            "55000/55000 [==============================] - 6s 107us/sample - loss: 0.2437 - acc: 0.9125 - val_loss: 0.2987 - val_acc: 0.8912\n",
            "Epoch 27/30\n",
            "55000/55000 [==============================] - 6s 105us/sample - loss: 0.2398 - acc: 0.9132 - val_loss: 0.3016 - val_acc: 0.8924\n",
            "Epoch 28/30\n",
            "55000/55000 [==============================] - 6s 103us/sample - loss: 0.2349 - acc: 0.9149 - val_loss: 0.2997 - val_acc: 0.8922\n",
            "Epoch 29/30\n",
            "55000/55000 [==============================] - 6s 109us/sample - loss: 0.2318 - acc: 0.9156 - val_loss: 0.3072 - val_acc: 0.8886\n",
            "Epoch 30/30\n",
            "55000/55000 [==============================] - 6s 107us/sample - loss: 0.2288 - acc: 0.9171 - val_loss: 0.2924 - val_acc: 0.8952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zecCHL8QCQd",
        "colab_type": "code",
        "outputId": "3318e6d2-170e-4940-fdd3-a70f0fada0d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "pd.DataFrame(model_fit.history).plot(figsize=(8,5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0,1) # plt.gca(): Get the current Axes instance on the current figure matching the given keyword args, or create one.\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAEzCAYAAAALosttAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfEbFWURExGdUnEVERHxGxVlERMRn/gcqCHHS3Zg9MgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwowOFo2VOPV",
        "colab_type": "code",
        "outputId": "3eac97e0-eb7a-4e92-cb80-e701843efd22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 46us/sample - loss: 59.2613 - acc: 0.8583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[59.26126955986023, 0.8583]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6T08TxByhGW",
        "colab_type": "text"
      },
      "source": [
        "#### Using the Model to Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88imSlQinV-e",
        "colab_type": "code",
        "outputId": "06d2a2c5-f1a4-4d00-c7c7-a22c72b0d8a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Using the Model to Make Predictions\n",
        "# Since we donâ€™t have actual new instances, we will just use the first 3 instances of the test set:\n",
        "X_new = X_test[:3]\n",
        "y_proba = model.predict(X_new)\n",
        "y_proba.round(2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P_jCsiBn8qd",
        "colab_type": "code",
        "outputId": "48e553ed-acd3-462a-81fc-56e1f7590432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# If you only care about the class with the highest estimated probability(even if that probability is quite low),\n",
        "# then you can use the predict_classes() method instead:\n",
        "y_pred = model.predict_classes(X_new)\n",
        "y_pred"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 2, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NKBDjGWoIyl",
        "colab_type": "code",
        "outputId": "fb8fc3a2-f813-4b67-bbbe-734e28976111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.array(class_names)[y_pred]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8htucGzbq39X",
        "colab_type": "code",
        "outputId": "771a0125-6df8-4d53-b8f8-cb4fd6012d21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#  the classifier actually classified all three images correctly:\n",
        "y_test[:3]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 2, 1], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byOyO3d7r3dB",
        "colab_type": "text"
      },
      "source": [
        "### Building a Regression MLP Using the Sequential API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_P-b9WArg-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use Scikit-Learnâ€™s fetch_california_housing() function to load the data:\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87x-TNnmLoff",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2d06ae9e-7745-4324-c6e3-d139f38ed67f"
      },
      "source": [
        "housing = fetch_california_housing()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n",
            "I0710 08:51:58.039066 139636251805568 california_housing.py:114] Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6A7krlxsrpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7AVzdn8LkOQ",
        "colab_type": "code",
        "outputId": "df2f22c1-0dd6-4d9e-bcdb-e8ada7e45c2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# StandardScaler(): Standardize features by removing the mean and scaling to unit variance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)  # fit_transform:ã€€Fit to data, then transform it.  So after this step, the scaler object has the mean and std attribute of X_train\n",
        "X_valid_scaled = scaler.transform(X_valid) # Using the mean and std attribute of X_train to standardize X_valid\n",
        "X_test_scaled = scaler.transform(X_test) # Using the mean and std attribute of X_train to standardize X_test\n",
        "print(scaler.mean_)\n",
        "print(np.mean(X_train[:,0]))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 3.86078633e+00  2.87113695e+01  5.43408177e+00  1.09832747e+00\n",
            "  1.41715926e+03  3.14697097e+00  3.56334625e+01 -1.19572295e+02]\n",
            "3.8607863307493546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEVfIsUBLkr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation = 'relu', input_shape = X_train.shape[1:]), # the input_shape is start from index 1 of the X_train.shape, since the index 0 is the sample size\n",
        "    keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wzlFxXLWNUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss = 'mean_squared_error', optimizer = 'sgd')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_PGfGjuMUpn",
        "colab_type": "code",
        "outputId": "1b403fde-969a-4c1f-a6fa-72bb098e24c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "model_fit = model.fit(X_train_scaled, y_train, epochs=20, validation_data = (X_valid_scaled, y_valid))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11610 samples, validate on 3870 samples\n",
            "Epoch 1/20\n",
            "11610/11610 [==============================] - 1s 75us/sample - loss: 0.9452 - val_loss: 0.5647\n",
            "Epoch 2/20\n",
            "11610/11610 [==============================] - 1s 44us/sample - loss: 0.5319 - val_loss: 0.4971\n",
            "Epoch 3/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4911 - val_loss: 0.4721\n",
            "Epoch 4/20\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4668 - val_loss: 0.4479\n",
            "Epoch 5/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4507 - val_loss: 0.4340\n",
            "Epoch 6/20\n",
            "11610/11610 [==============================] - 1s 43us/sample - loss: 0.4412 - val_loss: 0.4276\n",
            "Epoch 7/20\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4327 - val_loss: 0.4155\n",
            "Epoch 8/20\n",
            "11610/11610 [==============================] - 1s 44us/sample - loss: 0.4327 - val_loss: 0.4170\n",
            "Epoch 9/20\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4197 - val_loss: 0.4073\n",
            "Epoch 10/20\n",
            "11610/11610 [==============================] - 1s 44us/sample - loss: 0.4151 - val_loss: 0.4033\n",
            "Epoch 11/20\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4113 - val_loss: 0.4012\n",
            "Epoch 12/20\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4076 - val_loss: 0.3951\n",
            "Epoch 13/20\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4019 - val_loss: 0.3884\n",
            "Epoch 14/20\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3998 - val_loss: 0.3904\n",
            "Epoch 15/20\n",
            "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3984 - val_loss: 0.3837\n",
            "Epoch 16/20\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3979 - val_loss: 0.4110\n",
            "Epoch 17/20\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4036 - val_loss: 0.3809\n",
            "Epoch 18/20\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4695 - val_loss: 0.3918\n",
            "Epoch 19/20\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3984 - val_loss: 0.3926\n",
            "Epoch 20/20\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3911 - val_loss: 0.3824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDJJLPpARdZ_",
        "colab_type": "code",
        "outputId": "6ca9d6fd-2120-432c-82c9-1f1a696a2d81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mse_test = model.evaluate(X_test_scaled, y_test)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5160/5160 [==============================] - 0s 26us/sample - loss: 0.3931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-FxTWgST4ur",
        "colab_type": "code",
        "outputId": "46b7adef-dc9d-4db1-cbc7-26b555d7c747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "X_new = X_test_scaled[:3] # pretend these are new instances\n",
        "y_pred = model.predict(X_new)\n",
        "y_pred"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6941538],\n",
              "       [0.9407015],\n",
              "       [4.116179 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiIsmvDzjb2r",
        "colab_type": "text"
      },
      "source": [
        "### Building Complex Models Using the Functional API\n",
        "p.331"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPBdtoY1pe4U",
        "colab_type": "text"
      },
      "source": [
        "One example of a non-sequential neural network is a **Wide & Deep neural network**.\n",
        "It connects all or part of the inputs directly to the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_-Cl9fpWGzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Letâ€™s build such a neural network to tackle the California housing problem:\n",
        "input = keras.layers.Input(shape=X_train.shape[1:])\n",
        "hidden1 = keras.layers.Dense(30, activation = 'relu')(input)\n",
        "hidden2 = keras.layers.Dense(30, activation = 'relu')(hidden1)\n",
        "concat = keras.layers.concatenate([input, hidden2])\n",
        "output = keras.layers.Dense(1)(concat)\n",
        "model = keras.models.Model(inputs=[input], outputs=[output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYJG3EogpZFy",
        "colab_type": "text"
      },
      "source": [
        "if we want to send a subset of the features through the wide path, and a different subset (possibly overlapping) through the deep path (see Figure 10-14)? In this case, one solution is to use **multiple inputs**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd-ZBrWWW2VD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_A = keras.layers.Input(shape=[5])\n",
        "input_B = keras.layers.Input(shape=[6])\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input_A, hidden2])\n",
        "output = keras.layers.Dense(1)(concat)\n",
        "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "059di3dJZfs1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "4e11c92e-fc99-4653-f8a4-3a39cbb0aaf7"
      },
      "source": [
        "model.compile(loss='mse', optimizer='sgd')\n",
        "\n",
        "X_train_A, X_train_B = X_train[:,:5], X_train[:,2:]\n",
        "X_valid_A, X_valid_B = X_valid[:,:5], X_valid[:,2:]\n",
        "X_test_A, X_test_B = X_test[:,:5], X_test[:,2:]\n",
        "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
        "\n",
        "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
        "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
        "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
        "y_pred = model.predict((X_new_A, X_new_B))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11610 samples, validate on 3870 samples\n",
            "Epoch 1/20\n",
            "11610/11610 [==============================] - 1s 69us/sample - loss: nan - val_loss: nan\n",
            "Epoch 2/20\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: nan - val_loss: nan\n",
            "Epoch 3/20\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: nan - val_loss: nan\n",
            "Epoch 4/20\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: nan - val_loss: nan\n",
            "Epoch 5/20\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: nan - val_loss: nan\n",
            "Epoch 6/20\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: nan - val_loss: nan\n",
            "Epoch 7/20\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: nan - val_loss: nan\n",
            "Epoch 8/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: nan - val_loss: nan\n",
            "Epoch 9/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: nan - val_loss: nan\n",
            "Epoch 10/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: nan - val_loss: nan\n",
            "Epoch 11/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: nan - val_loss: nan\n",
            "Epoch 12/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: nan - val_loss: nan\n",
            "Epoch 13/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: nan - val_loss: nan\n",
            "Epoch 14/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: nan - val_loss: nan\n",
            "Epoch 15/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: nan - val_loss: nan\n",
            "Epoch 16/20\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: nan - val_loss: nan\n",
            "Epoch 17/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: nan - val_loss: nan\n",
            "Epoch 18/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: nan - val_loss: nan\n",
            "Epoch 19/20\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: nan - val_loss: nan\n",
            "Epoch 20/20\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: nan - val_loss: nan\n",
            "5160/5160 [==============================] - 0s 24us/sample - loss: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsivkeJb3w0a",
        "colab_type": "text"
      },
      "source": [
        "p.334   ...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXQcJUudfvGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvM0bAI539mL",
        "colab_type": "text"
      },
      "source": [
        "### Building Dynamic Models Using the Subclassing API\n",
        "p.336"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eug3zEtQsBgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3pTrCMc4CDm",
        "colab_type": "text"
      },
      "source": [
        "### Saving and Restoring a Model\n",
        "p.337"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F-X1EWX7b9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving a trained Keras model is as simple as it gets:\n",
        "model.save(\"my_keras_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7C6GXPG7riv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the model\n",
        "model = keras.models.load_model(\"my_keras_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNsokpwt4OgS",
        "colab_type": "text"
      },
      "source": [
        "### Using Callbacks\n",
        "The `fit()` method accepts a **callbacks argument** that lets you specify a list of objects\n",
        "that Keras will call *during training at the start and end of training*, *at the start and end\n",
        "of each epoch* and even *before and after processing each batch*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kqzfnixl4O0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [...] # build and compile the model\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\n",
        "history = model.fit(X_train, y_train, epochs=10, callbacks =[checkpoint_cb])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2Br_arr8Q6v",
        "colab_type": "text"
      },
      "source": [
        "set **`save_best_only=True`** when creating the **`ModelCheckpoint`**. In this case, it will only\n",
        "save your model when its performance on the validation set is the best so far."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzozeEdw8goQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",\n",
        "                                                set_best_only = True)\n",
        "history = model.fit(X_train, y_train, epochs = 10, \n",
        "                    validation_data = (X_valid, y_valid),\n",
        "                    callbacks = [checkpoint_cb])\n",
        "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZfp-9Sn8gwJ",
        "colab_type": "text"
      },
      "source": [
        "Another way to implement early stopping is to simply use the **`EarlyStopping`** callâ€\n",
        "back. It will interrupt training when it measures no progress on the validation set for\n",
        "a number of epochs (defined by the **`patience`** argument), and it will optionally roll\n",
        "back to the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32wTyLMK8g2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                  restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=100,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B1SY-bRsBwq",
        "colab_type": "text"
      },
      "source": [
        "###Visualization Using TensorBoard\n",
        "p.339"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2ylnC1bsCzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "root_logdir = os.path.join(os.curdir, 'my_logs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo18Tfk4sOMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_run_logdir():\n",
        "    import time\n",
        "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "    return os.path.join(root_logdir, run_id)\n",
        "  \n",
        "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_01_16-11_28_43'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb9-ZnWpseTd",
        "colab_type": "code",
        "outputId": "b290489e-511f-4e3f-faa1-f22437e3bb74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# [...] # Build and compile your model\n",
        "\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
        "history = model.fit(X_train, y_train, epochs=30,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[tensorboard_cb])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 55000 samples, validate on 5000 samples\n",
            "Epoch 1/30\n",
            "55000/55000 [==============================] - 7s 119us/sample - loss: 0.7207 - acc: 0.7658 - val_loss: 0.5258 - val_acc: 0.8250\n",
            "Epoch 2/30\n",
            "55000/55000 [==============================] - 6s 113us/sample - loss: 0.4900 - acc: 0.8283 - val_loss: 0.4537 - val_acc: 0.8426\n",
            "Epoch 3/30\n",
            "55000/55000 [==============================] - 6s 117us/sample - loss: 0.4428 - acc: 0.8450 - val_loss: 0.4164 - val_acc: 0.8584\n",
            "Epoch 4/30\n",
            "55000/55000 [==============================] - 6s 114us/sample - loss: 0.4141 - acc: 0.8538 - val_loss: 0.4006 - val_acc: 0.8610\n",
            "Epoch 5/30\n",
            "55000/55000 [==============================] - 7s 119us/sample - loss: 0.3933 - acc: 0.8623 - val_loss: 0.3828 - val_acc: 0.8672\n",
            "Epoch 6/30\n",
            "55000/55000 [==============================] - 6s 115us/sample - loss: 0.3788 - acc: 0.8661 - val_loss: 0.3762 - val_acc: 0.8690\n",
            "Epoch 7/30\n",
            "55000/55000 [==============================] - 7s 120us/sample - loss: 0.3648 - acc: 0.8707 - val_loss: 0.3672 - val_acc: 0.8710\n",
            "Epoch 8/30\n",
            "55000/55000 [==============================] - 6s 114us/sample - loss: 0.3541 - acc: 0.8743 - val_loss: 0.3670 - val_acc: 0.8694\n",
            "Epoch 9/30\n",
            "55000/55000 [==============================] - 6s 114us/sample - loss: 0.3427 - acc: 0.8772 - val_loss: 0.3558 - val_acc: 0.8760\n",
            "Epoch 10/30\n",
            "55000/55000 [==============================] - 7s 122us/sample - loss: 0.3339 - acc: 0.8810 - val_loss: 0.3588 - val_acc: 0.8750\n",
            "Epoch 11/30\n",
            "55000/55000 [==============================] - 7s 128us/sample - loss: 0.3259 - acc: 0.8833 - val_loss: 0.3463 - val_acc: 0.8776\n",
            "Epoch 12/30\n",
            "55000/55000 [==============================] - 6s 116us/sample - loss: 0.3179 - acc: 0.8860 - val_loss: 0.3419 - val_acc: 0.8758\n",
            "Epoch 13/30\n",
            "55000/55000 [==============================] - 6s 114us/sample - loss: 0.3111 - acc: 0.8889 - val_loss: 0.3256 - val_acc: 0.8830\n",
            "Epoch 14/30\n",
            "55000/55000 [==============================] - 6s 117us/sample - loss: 0.3032 - acc: 0.8915 - val_loss: 0.3261 - val_acc: 0.8824\n",
            "Epoch 15/30\n",
            "55000/55000 [==============================] - 7s 121us/sample - loss: 0.2974 - acc: 0.8932 - val_loss: 0.3367 - val_acc: 0.8780\n",
            "Epoch 16/30\n",
            "55000/55000 [==============================] - 7s 125us/sample - loss: 0.2911 - acc: 0.8960 - val_loss: 0.3499 - val_acc: 0.8706\n",
            "Epoch 17/30\n",
            "55000/55000 [==============================] - 7s 121us/sample - loss: 0.2856 - acc: 0.8976 - val_loss: 0.3218 - val_acc: 0.8840\n",
            "Epoch 18/30\n",
            "55000/55000 [==============================] - 6s 111us/sample - loss: 0.2803 - acc: 0.8987 - val_loss: 0.3214 - val_acc: 0.8848\n",
            "Epoch 19/30\n",
            "55000/55000 [==============================] - 6s 117us/sample - loss: 0.2743 - acc: 0.9017 - val_loss: 0.3138 - val_acc: 0.8876\n",
            "Epoch 20/30\n",
            "55000/55000 [==============================] - 6s 116us/sample - loss: 0.2695 - acc: 0.9027 - val_loss: 0.3148 - val_acc: 0.8850\n",
            "Epoch 21/30\n",
            "55000/55000 [==============================] - 7s 120us/sample - loss: 0.2647 - acc: 0.9048 - val_loss: 0.3059 - val_acc: 0.8880\n",
            "Epoch 22/30\n",
            "55000/55000 [==============================] - 6s 117us/sample - loss: 0.2591 - acc: 0.9069 - val_loss: 0.3157 - val_acc: 0.8844\n",
            "Epoch 23/30\n",
            "55000/55000 [==============================] - 7s 121us/sample - loss: 0.2550 - acc: 0.9077 - val_loss: 0.3065 - val_acc: 0.8844\n",
            "Epoch 24/30\n",
            "55000/55000 [==============================] - 7s 120us/sample - loss: 0.2503 - acc: 0.9104 - val_loss: 0.3146 - val_acc: 0.8854\n",
            "Epoch 25/30\n",
            "55000/55000 [==============================] - 7s 123us/sample - loss: 0.2464 - acc: 0.9113 - val_loss: 0.2991 - val_acc: 0.8928\n",
            "Epoch 26/30\n",
            "55000/55000 [==============================] - 6s 117us/sample - loss: 0.2428 - acc: 0.9118 - val_loss: 0.3108 - val_acc: 0.8898\n",
            "Epoch 27/30\n",
            "55000/55000 [==============================] - 6s 109us/sample - loss: 0.2387 - acc: 0.9140 - val_loss: 0.3037 - val_acc: 0.8920\n",
            "Epoch 28/30\n",
            "55000/55000 [==============================] - 6s 113us/sample - loss: 0.2343 - acc: 0.9153 - val_loss: 0.2988 - val_acc: 0.8882\n",
            "Epoch 29/30\n",
            "55000/55000 [==============================] - 6s 116us/sample - loss: 0.2300 - acc: 0.9167 - val_loss: 0.3127 - val_acc: 0.8890\n",
            "Epoch 30/30\n",
            "55000/55000 [==============================] - 7s 124us/sample - loss: 0.2264 - acc: 0.9185 - val_loss: 0.2985 - val_acc: 0.8930\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVqUbJ2bsrFh",
        "colab_type": "code",
        "outputId": "a5130783-2963-48aa-a8df-d8a8fa6a4839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!tensorboard --logdir=./my_logs --port=6006\n",
        "!TensorBoard 2.0.0 at http://mycomputer.local:6006"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorBoard 1.14.0 at http://c6baa84c8a09:6006/ (Press CTRL+C to quit)\n",
            "^C\n",
            "/bin/bash: TensorBoard: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D00CL6puTZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2KmBSr6v0XY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMXwbSUfvzy4",
        "colab_type": "text"
      },
      "source": [
        "## Fine-Tuning Neural Network Hyperparameters\n",
        "p.342"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L29laYnjxHxH",
        "colab_type": "text"
      },
      "source": [
        "The flexibility of neural networks is also one of their main drawbacks: there are many\n",
        "hyperparameters to tweak.  How do you know what combination of hyperparaâ€\n",
        "meters is the best for your task?\n",
        "One option is to simply try many combinations of hyperparameters and see which\n",
        "one works best on the validation set (or using K-fold cross-validation).\n",
        "For this, one approach is simply use **GridSearchCV** or **RandomizedSearchCV** to explore the hyperâ€\n",
        "parameter space. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcjSB7jqxc8D",
        "colab_type": "text"
      },
      "source": [
        "For this, we need to wrap our Keras models in objects that mimic regular Scikit-Learn regressors. \n",
        "</br>The first step is to create a function that will build and compile a Keras model, given a set of hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pck-Az9Mv2p0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
        "  model = keras.models.Sequential()\n",
        "  options = {'input_shape': input_shape} # The options dict is used to ensure that the first layer is properly given the input shape\n",
        "  for layers in range(n_hidden):\n",
        "    model.add(keras.layers.Dense(n_neurons, activation='relu', **options))\n",
        "    options={}\n",
        "  model.add(keras.layers.Dense(1, **options))\n",
        "  optimizer = keras.optimizers.SGD(learning_rate)\n",
        "  model.compile(loss='mse', optimizer=optimizer)\n",
        "  return model\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3e3yT-lD5cX",
        "colab_type": "text"
      },
      "source": [
        "Next, letâ€™s create a KerasRegressor based on this build_model() function:\n",
        "<br/>The KerasRegressor object is a thin wrapper around the Keras model built using\n",
        "build_model(). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PeQnC0iDuLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtrteuEPGptt",
        "colab_type": "text"
      },
      "source": [
        " it will just use the default hyperparameters we defined in build_model(). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9CFJN0ZERk-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "f915cc17-8ac8-490f-8cba-1608b4cdf840"
      },
      "source": [
        "keras_reg.fit(X_train_scaled, y_train, epochs=10,\n",
        "              validation_data=(X_valid_scaled, y_valid),\n",
        "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
        "mse_test = keras_reg.score(X_test_scaled, y_test)\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11610 samples, validate on 3870 samples\n",
            "Epoch 1/10\n",
            "11610/11610 [==============================] - 1s 79us/sample - loss: 1.2247 - val_loss: 0.7009\n",
            "Epoch 2/10\n",
            "11610/11610 [==============================] - 1s 43us/sample - loss: 0.6399 - val_loss: 0.5999\n",
            "Epoch 3/10\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5742 - val_loss: 0.5514\n",
            "Epoch 4/10\n",
            "11610/11610 [==============================] - 1s 43us/sample - loss: 0.5389 - val_loss: 0.5191\n",
            "Epoch 5/10\n",
            "11610/11610 [==============================] - 1s 43us/sample - loss: 0.5297 - val_loss: 0.5050\n",
            "Epoch 6/10\n",
            "11610/11610 [==============================] - 1s 44us/sample - loss: 0.5021 - val_loss: 0.4866\n",
            "Epoch 7/10\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4915 - val_loss: 0.4767\n",
            "Epoch 8/10\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4830 - val_loss: 0.4690\n",
            "Epoch 9/10\n",
            "11610/11610 [==============================] - 0s 43us/sample - loss: 0.4774 - val_loss: 0.4627\n",
            "Epoch 10/10\n",
            "11610/11610 [==============================] - 1s 43us/sample - loss: 0.4725 - val_loss: 0.4564\n",
            "5160/5160 [==============================] - 0s 23us/sample - loss: 0.4653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7ygnX7EHTkV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "958dd1e5-304d-48f2-8dbf-8a3cd8399acd"
      },
      "source": [
        "X_new = X_test_scaled[:3]\n",
        "y_pred = keras_reg.predict(X_new)\n",
        "y_pred"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.1040471 , 0.98380256, 3.876338  ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAK4OrmHJHqU",
        "colab_type": "text"
      },
      "source": [
        "we want to train hundreds of variants and see which one performs best on the validation\n",
        "set. \n",
        "<br/>Since there are many hyperparameters, it is preferable to use a randomized search\n",
        "rather than grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-jadRTKI5gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAYaNmWyJjMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_distribs = {\n",
        "    \"n_hidden\":[0,1,2,3],\n",
        "    \"n_neurons\":np.arange(1,100),\n",
        "    \"learning_rate\":reciprocal(3e-4, 3e-2),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAK7KL6UKvlA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3aff565c-85c2-427e-f0e8-02354e03e16b"
      },
      "source": [
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
        "rnd_search_cv.fit(X_train_scaled, y_train, epochs=30,\n",
        "                  validation_data=(X_valid_scaled, y_valid),\n",
        "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 1s 114us/sample - loss: 3.6584 - val_loss: 3.0218\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 2.4571 - val_loss: 2.2322\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 1.8915 - val_loss: 1.7665\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 1.5261 - val_loss: 1.4468\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 1.2723 - val_loss: 1.2214\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 1.0961 - val_loss: 1.0669\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.9744 - val_loss: 0.9593\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.8896 - val_loss: 0.8854\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.8310 - val_loss: 0.8338\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.7911 - val_loss: 0.7989\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.7636 - val_loss: 0.7741\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.7439 - val_loss: 0.7558\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7292 - val_loss: 0.7419\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.7177 - val_loss: 0.7313\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.7086 - val_loss: 0.7226\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.7013 - val_loss: 0.7152\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6949 - val_loss: 0.7088\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6897 - val_loss: 0.7034\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6848 - val_loss: 0.6982\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6804 - val_loss: 0.6934\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6761 - val_loss: 0.6889\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6723 - val_loss: 0.6850\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6686 - val_loss: 0.6808\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6650 - val_loss: 0.6769\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6614 - val_loss: 0.6729\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6581 - val_loss: 0.6691\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6546 - val_loss: 0.6656\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6510 - val_loss: 0.6623\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6478 - val_loss: 0.6580\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6444 - val_loss: 0.6546\n",
            "3870/3870 [==============================] - 0s 24us/sample - loss: 0.6722\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 1s 128us/sample - loss: 5.5868 - val_loss: 4.2497\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 3.5442 - val_loss: 3.1225\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 2.6953 - val_loss: 2.4730\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 2.1887 - val_loss: 2.0697\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 1.8737 - val_loss: 1.8151\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 1.6765 - val_loss: 1.6537\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 1.5521 - val_loss: 1.5503\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 1.4737 - val_loss: 1.4842\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 1.4241 - val_loss: 1.4415\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 1.3925 - val_loss: 1.4139\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 1.3726 - val_loss: 1.3960\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 1.3598 - val_loss: 1.3841\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 1.3516 - val_loss: 1.3764\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 1.3464 - val_loss: 1.3712\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 1.3431 - val_loss: 1.3678\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 1.3410 - val_loss: 1.3654\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 1.3395 - val_loss: 1.3637\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 1.3386 - val_loss: 1.3626\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 1.3379 - val_loss: 1.3618\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 1.3374 - val_loss: 1.3612\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 1.3371 - val_loss: 1.3607\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 1.3368 - val_loss: 1.3604\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 1.3366 - val_loss: 1.3601\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 1.3364 - val_loss: 1.3599\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 1.3363 - val_loss: 1.3598\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 1.3362 - val_loss: 1.3596\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 1.3361 - val_loss: 1.3595\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 1.3360 - val_loss: 1.3595\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 1.3359 - val_loss: 1.3594\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 1.3358 - val_loss: 1.3593\n",
            "3870/3870 [==============================] - 0s 24us/sample - loss: 1.3280\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 1s 132us/sample - loss: 4.0942 - val_loss: 2.7431\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 1.9539 - val_loss: 1.4014\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 1.1468 - val_loss: 0.9801\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.9128 - val_loss: 0.8464\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.8161 - val_loss: 0.7691\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.7524 - val_loss: 0.7162\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.7087 - val_loss: 0.6799\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6796 - val_loss: 0.6558\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6597 - val_loss: 0.6384\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6449 - val_loss: 0.6250\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6334 - val_loss: 0.6143\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6241 - val_loss: 0.6048\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6160 - val_loss: 0.5968\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6087 - val_loss: 0.5890\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6021 - val_loss: 0.5827\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5963 - val_loss: 0.5769\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5912 - val_loss: 0.5716\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5866 - val_loss: 0.5668\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5821 - val_loss: 0.5622\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5781 - val_loss: 0.5580\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5742 - val_loss: 0.5540\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5704 - val_loss: 0.5503\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5670 - val_loss: 0.5467\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5635 - val_loss: 0.5433\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5603 - val_loss: 0.5399\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5572 - val_loss: 0.5367\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5545 - val_loss: 0.5338\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5518 - val_loss: 0.5308\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5493 - val_loss: 0.5280\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5469 - val_loss: 0.5252\n",
            "3870/3870 [==============================] - 0s 25us/sample - loss: 0.5216\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 1s 141us/sample - loss: 2.7145 - val_loss: 1.2967\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 1.0691 - val_loss: 0.9528\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.9047 - val_loss: 0.8788\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.8407 - val_loss: 0.8345\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.7999 - val_loss: 0.8000\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.7684 - val_loss: 0.7720\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.7420 - val_loss: 0.7496\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.7211 - val_loss: 0.7287\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.7014 - val_loss: 0.7102\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6841 - val_loss: 0.6938\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6686 - val_loss: 0.6785\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6542 - val_loss: 0.6645\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6409 - val_loss: 0.6514\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6289 - val_loss: 0.6392\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6178 - val_loss: 0.6276\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6076 - val_loss: 0.6166\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5978 - val_loss: 0.6062\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5887 - val_loss: 0.5966\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5803 - val_loss: 0.5875\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5722 - val_loss: 0.5790\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5647 - val_loss: 0.5708\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5574 - val_loss: 0.5633\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5511 - val_loss: 0.5559\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5447 - val_loss: 0.5491\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5388 - val_loss: 0.5429\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5334 - val_loss: 0.5368\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5282 - val_loss: 0.5310\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5234 - val_loss: 0.5256\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5189 - val_loss: 0.5205\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5147 - val_loss: 0.5159\n",
            "3870/3870 [==============================] - 0s 25us/sample - loss: 0.5394\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 1s 145us/sample - loss: 2.7547 - val_loss: 1.3957\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 1.0392 - val_loss: 0.8612\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.7723 - val_loss: 0.7478\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.7030 - val_loss: 0.7049\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6727 - val_loss: 0.6813\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.6542 - val_loss: 0.6634\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6397 - val_loss: 0.6496\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.6276 - val_loss: 0.6367\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6167 - val_loss: 0.6248\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6067 - val_loss: 0.6142\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5974 - val_loss: 0.6037\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5887 - val_loss: 0.5950\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5806 - val_loss: 0.5864\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5729 - val_loss: 0.5779\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5657 - val_loss: 0.5701\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5590 - val_loss: 0.5628\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5526 - val_loss: 0.5555\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5466 - val_loss: 0.5492\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5410 - val_loss: 0.5426\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5356 - val_loss: 0.5368\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5305 - val_loss: 0.5311\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5256 - val_loss: 0.5258\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5210 - val_loss: 0.5209\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5168 - val_loss: 0.5161\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5127 - val_loss: 0.5118\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5089 - val_loss: 0.5075\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5053 - val_loss: 0.5036\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5017 - val_loss: 0.5000\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4985 - val_loss: 0.4964\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4954 - val_loss: 0.4933\n",
            "3870/3870 [==============================] - 0s 25us/sample - loss: 0.5028\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 1s 163us/sample - loss: 2.8984 - val_loss: 1.5954\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 1.1556 - val_loss: 0.8621\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.7756 - val_loss: 0.7319\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6909 - val_loss: 0.6900\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6628 - val_loss: 0.6681\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6468 - val_loss: 0.6527\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6346 - val_loss: 0.6391\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6237 - val_loss: 0.6272\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6140 - val_loss: 0.6162\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6051 - val_loss: 0.6066\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5970 - val_loss: 0.5973\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5893 - val_loss: 0.5889\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5822 - val_loss: 0.5802\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5758 - val_loss: 0.5731\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5696 - val_loss: 0.5660\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5640 - val_loss: 0.5598\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5585 - val_loss: 0.5535\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5535 - val_loss: 0.5481\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5486 - val_loss: 0.5433\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5440 - val_loss: 0.5379\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5397 - val_loss: 0.5327\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5360 - val_loss: 0.5283\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5322 - val_loss: 0.5242\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5287 - val_loss: 0.5202\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5254 - val_loss: 0.5165\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5223 - val_loss: 0.5129\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5192 - val_loss: 0.5099\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5165 - val_loss: 0.5064\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5136 - val_loss: 0.5036\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5111 - val_loss: 0.5008\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 0.4975\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 1s 151us/sample - loss: 6.0104 - val_loss: 5.3226\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 4.3119 - val_loss: 3.8730\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 3.1688 - val_loss: 2.8869\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 2.3917 - val_loss: 2.2101\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 1.8593 - val_loss: 1.7434\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 1.4929 - val_loss: 1.4197\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 1.2389 - val_loss: 1.1932\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 1.0617 - val_loss: 1.0344\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.9377 - val_loss: 0.9221\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.8502 - val_loss: 0.8423\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7881 - val_loss: 0.7849\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.7433 - val_loss: 0.7433\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.7110 - val_loss: 0.7128\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6874 - val_loss: 0.6902\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.6697 - val_loss: 0.6732\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6563 - val_loss: 0.6602\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.6460 - val_loss: 0.6500\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 0.6376 - val_loss: 0.6419\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.6314 - val_loss: 0.6353\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.6261 - val_loss: 0.6298\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 0.6215 - val_loss: 0.6252\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6176 - val_loss: 0.6212\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.6141 - val_loss: 0.6176\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6110 - val_loss: 0.6144\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.6082 - val_loss: 0.6115\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 0.6057 - val_loss: 0.6088\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.6032 - val_loss: 0.6062\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.6010 - val_loss: 0.6039\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5988 - val_loss: 0.6016\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5968 - val_loss: 0.5995\n",
            "3870/3870 [==============================] - 0s 22us/sample - loss: 0.6280\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 1s 153us/sample - loss: 5.9478 - val_loss: 4.7892\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 4.1083 - val_loss: 3.4129\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 2.9200 - val_loss: 2.4922\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 2.1435 - val_loss: 1.8733\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 1.6294 - val_loss: 1.4545\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 1.2844 - val_loss: 1.1703\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 1.0532 - val_loss: 0.9776\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.8976 - val_loss: 0.8465\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.7924 - val_loss: 0.7567\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.7207 - val_loss: 0.6953\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 0.6719 - val_loss: 0.6530\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.6386 - val_loss: 0.6238\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.6157 - val_loss: 0.6036\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5999 - val_loss: 0.5894\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5888 - val_loss: 0.5793\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5810 - val_loss: 0.5720\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5753 - val_loss: 0.5666\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5712 - val_loss: 0.5626\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5681 - val_loss: 0.5596\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5657 - val_loss: 0.5571\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5638 - val_loss: 0.5551\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5621 - val_loss: 0.5534\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5608 - val_loss: 0.5520\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5597 - val_loss: 0.5507\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5586 - val_loss: 0.5496\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5577 - val_loss: 0.5485\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5568 - val_loss: 0.5475\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5560 - val_loss: 0.5465\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5552 - val_loss: 0.5456\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5544 - val_loss: 0.5448\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 0.5539\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 1s 157us/sample - loss: 6.3380 - val_loss: 5.1961\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 4.3512 - val_loss: 3.6334\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 3.0781 - val_loss: 2.6182\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 2.2503 - val_loss: 1.9505\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 1.7058 - val_loss: 1.5078\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 1.3448 - val_loss: 1.2116\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 1.1031 - val_loss: 1.0124\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.9406 - val_loss: 0.8777\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.8306 - val_loss: 0.7856\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.7556 - val_loss: 0.7226\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.7042 - val_loss: 0.6792\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.6688 - val_loss: 0.6492\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 0.6442 - val_loss: 0.6279\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.6268 - val_loss: 0.6129\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.6145 - val_loss: 0.6022\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 0.6056 - val_loss: 0.5941\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5990 - val_loss: 0.5881\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5940 - val_loss: 0.5836\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 0.5902 - val_loss: 0.5799\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5871 - val_loss: 0.5770\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5846 - val_loss: 0.5746\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5825 - val_loss: 0.5724\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 0.5806 - val_loss: 0.5705\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 0.5789 - val_loss: 0.5689\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 0.5775 - val_loss: 0.5673\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 43us/sample - loss: 0.5760 - val_loss: 0.5658\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 0.5748 - val_loss: 0.5645\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 0.5736 - val_loss: 0.5632\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5724 - val_loss: 0.5619\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5713 - val_loss: 0.5607\n",
            "3870/3870 [==============================] - 0s 21us/sample - loss: 0.5579\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 1s 165us/sample - loss: 4.2007 - val_loss: 2.1858\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 1.4224 - val_loss: 0.9562\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 0.7826 - val_loss: 0.6665\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.6233 - val_loss: 0.5920\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5781 - val_loss: 0.5686\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5627 - val_loss: 0.5596\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5557 - val_loss: 0.5544\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5511 - val_loss: 0.5510\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5483 - val_loss: 0.5478\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5455 - val_loss: 0.5442\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5433 - val_loss: 0.5415\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5412 - val_loss: 0.5396\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5397 - val_loss: 0.5371\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5381 - val_loss: 0.5353\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5368 - val_loss: 0.5333\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5356 - val_loss: 0.5315\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5348 - val_loss: 0.5302\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5340 - val_loss: 0.5290\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5333 - val_loss: 0.5277\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5323 - val_loss: 0.5269\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5315 - val_loss: 0.5261\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5317 - val_loss: 0.5250\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5308 - val_loss: 0.5238\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5305 - val_loss: 0.5231\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5294 - val_loss: 0.5226\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5288 - val_loss: 0.5218\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5283 - val_loss: 0.5214\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 0.5286 - val_loss: 0.5211\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5294 - val_loss: 0.5203\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5287 - val_loss: 0.5200\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 0.5470\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 1s 172us/sample - loss: 3.3151 - val_loss: 1.8063\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 1.2435 - val_loss: 0.9103\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7865 - val_loss: 0.7016\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6710 - val_loss: 0.6407\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6337 - val_loss: 0.6164\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6164 - val_loss: 0.6032\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.6055 - val_loss: 0.5931\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5964 - val_loss: 0.5843\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5890 - val_loss: 0.5770\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5823 - val_loss: 0.5704\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5764 - val_loss: 0.5647\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5712 - val_loss: 0.5593\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5666 - val_loss: 0.5545\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5626 - val_loss: 0.5503\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5590 - val_loss: 0.5466\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5558 - val_loss: 0.5433\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5531 - val_loss: 0.5405\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5505 - val_loss: 0.5376\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5480 - val_loss: 0.5351\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5465 - val_loss: 0.5330\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5449 - val_loss: 0.5312\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5431 - val_loss: 0.5294\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5420 - val_loss: 0.5279\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5408 - val_loss: 0.5264\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 0.5394 - val_loss: 0.5251\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5389 - val_loss: 0.5240\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5379 - val_loss: 0.5230\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5371 - val_loss: 0.5220\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5364 - val_loss: 0.5212\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5358 - val_loss: 0.5204\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 0.5358\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 1s 172us/sample - loss: 3.7963 - val_loss: 2.1215\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 1.4645 - val_loss: 1.0765\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.9014 - val_loss: 0.7915\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.7430 - val_loss: 0.7032\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 0.6889 - val_loss: 0.6694\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6637 - val_loss: 0.6489\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6478 - val_loss: 0.6358\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.6352 - val_loss: 0.6242\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6248 - val_loss: 0.6130\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6156 - val_loss: 0.6049\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.6073 - val_loss: 0.5958\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6000 - val_loss: 0.5880\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5933 - val_loss: 0.5814\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5872 - val_loss: 0.5749\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5816 - val_loss: 0.5698\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5769 - val_loss: 0.5650\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5723 - val_loss: 0.5606\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5683 - val_loss: 0.5550\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5646 - val_loss: 0.5509\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5615 - val_loss: 0.5474\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5587 - val_loss: 0.5453\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5559 - val_loss: 0.5425\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5536 - val_loss: 0.5401\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5513 - val_loss: 0.5381\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5491 - val_loss: 0.5350\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5477 - val_loss: 0.5334\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5459 - val_loss: 0.5318\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5445 - val_loss: 0.5307\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5433 - val_loss: 0.5295\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5420 - val_loss: 0.5287\n",
            "3870/3870 [==============================] - 0s 22us/sample - loss: 0.5412\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 1s 189us/sample - loss: 1.3792 - val_loss: 0.7762\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.6959 - val_loss: 0.6884\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6409 - val_loss: 0.6377\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6033 - val_loss: 0.6042\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5751 - val_loss: 0.5763\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5521 - val_loss: 0.5519\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5346 - val_loss: 0.5336\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5204 - val_loss: 0.5201\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5080 - val_loss: 0.5083\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5000 - val_loss: 0.4992\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4922 - val_loss: 0.4929\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4864 - val_loss: 0.4850\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4805 - val_loss: 0.4799\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4759 - val_loss: 0.4747\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4714 - val_loss: 0.4711\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4683 - val_loss: 0.4662\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4642 - val_loss: 0.4631\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4619 - val_loss: 0.4606\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4588 - val_loss: 0.4578\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4569 - val_loss: 0.4549\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4541 - val_loss: 0.4517\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4520 - val_loss: 0.4490\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4504 - val_loss: 0.4476\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4481 - val_loss: 0.4455\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4463 - val_loss: 0.4437\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4445 - val_loss: 0.4406\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4426 - val_loss: 0.4388\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4417 - val_loss: 0.4370\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4394 - val_loss: 0.4355\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4383 - val_loss: 0.4332\n",
            "3870/3870 [==============================] - 0s 25us/sample - loss: 0.4498\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 1s 187us/sample - loss: 1.9698 - val_loss: 0.8337\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.9177 - val_loss: 0.6696\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.7317 - val_loss: 0.5962\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5929 - val_loss: 0.5666\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5509 - val_loss: 0.5399\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5309 - val_loss: 0.5183\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5137 - val_loss: 0.5045\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5028 - val_loss: 0.4928\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4945 - val_loss: 0.4835\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4857 - val_loss: 0.4766\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4800 - val_loss: 0.4712\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4749 - val_loss: 0.4657\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4715 - val_loss: 0.4617\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4663 - val_loss: 0.4611\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4641 - val_loss: 0.4551\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4607 - val_loss: 0.4518\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4577 - val_loss: 0.4493\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4555 - val_loss: 0.4473\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4531 - val_loss: 0.4451\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4502 - val_loss: 0.4424\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4489 - val_loss: 0.4413\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4465 - val_loss: 0.4393\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4445 - val_loss: 0.4373\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4428 - val_loss: 0.4350\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4406 - val_loss: 0.4339\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4394 - val_loss: 0.4323\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4374 - val_loss: 0.4316\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4361 - val_loss: 0.4292\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4343 - val_loss: 0.4296\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4327 - val_loss: 0.4273\n",
            "3870/3870 [==============================] - 0s 24us/sample - loss: 0.4475\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 2s 224us/sample - loss: 1.6812 - val_loss: 0.7006\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6655 - val_loss: 0.6481\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.6259 - val_loss: 0.6098\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5957 - val_loss: 0.5824\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5728 - val_loss: 0.5598\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5553 - val_loss: 0.5412\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5408 - val_loss: 0.5265\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5288 - val_loss: 0.5140\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5187 - val_loss: 0.5032\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5108 - val_loss: 0.4956\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5044 - val_loss: 0.4888\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4977 - val_loss: 0.4819\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4931 - val_loss: 0.4777\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4889 - val_loss: 0.4714\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4845 - val_loss: 0.4675\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4803 - val_loss: 0.4632\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4765 - val_loss: 0.4590\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4751 - val_loss: 0.4571\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4719 - val_loss: 0.4548\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4679 - val_loss: 0.4506\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4653 - val_loss: 0.4485\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4621 - val_loss: 0.4477\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4599 - val_loss: 0.4436\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4597 - val_loss: 0.4419\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4561 - val_loss: 0.4394\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4532 - val_loss: 0.4380\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4513 - val_loss: 0.4360\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4498 - val_loss: 0.4326\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4481 - val_loss: 0.4319\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4458 - val_loss: 0.4304\n",
            "3870/3870 [==============================] - 0s 24us/sample - loss: 0.4365\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 2s 199us/sample - loss: 2.5660 - val_loss: 1.0414\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.7578 - val_loss: 0.6412\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6024 - val_loss: 0.5905\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5771 - val_loss: 0.5752\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5655 - val_loss: 0.5664\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5586 - val_loss: 0.5589\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5536 - val_loss: 0.5524\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5491 - val_loss: 0.5452\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5456 - val_loss: 0.5413\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5412 - val_loss: 0.5382\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5399 - val_loss: 0.5353\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5390 - val_loss: 0.5332\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5364 - val_loss: 0.5290\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5341 - val_loss: 0.5285\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5333 - val_loss: 0.5260\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5356 - val_loss: 0.5242\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5326 - val_loss: 0.5242\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5280 - val_loss: 0.5233\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5303 - val_loss: 0.5218\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5297 - val_loss: 0.5205\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5321 - val_loss: 0.5207\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5320 - val_loss: 0.5185\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5307 - val_loss: 0.5192\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5267 - val_loss: 0.5194\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5331 - val_loss: 0.5186\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5322 - val_loss: 0.5182\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5293 - val_loss: 0.5185\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5313 - val_loss: 0.5173\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5295 - val_loss: 0.5188\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5354 - val_loss: 0.5169\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 0.5496\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 2s 200us/sample - loss: 2.9953 - val_loss: 1.2038\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.9807 - val_loss: 0.7967\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7668 - val_loss: 0.7274\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7092 - val_loss: 0.6899\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6782 - val_loss: 0.6621\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6518 - val_loss: 0.6384\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6314 - val_loss: 0.6180\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6146 - val_loss: 0.6017\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6008 - val_loss: 0.5885\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5888 - val_loss: 0.5775\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5800 - val_loss: 0.5668\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5720 - val_loss: 0.5592\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5661 - val_loss: 0.5531\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5602 - val_loss: 0.5473\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5561 - val_loss: 0.5426\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5511 - val_loss: 0.5385\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5481 - val_loss: 0.5345\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5477 - val_loss: 0.5318\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5445 - val_loss: 0.5292\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5436 - val_loss: 0.5264\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5404 - val_loss: 0.5255\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5424 - val_loss: 0.5234\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5395 - val_loss: 0.5224\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5390 - val_loss: 0.5211\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5384 - val_loss: 0.5197\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5392 - val_loss: 0.5189\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5352 - val_loss: 0.5190\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5397 - val_loss: 0.5177\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5368 - val_loss: 0.5183\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5367 - val_loss: 0.5166\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 0.5371\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 2s 204us/sample - loss: 2.7373 - val_loss: 1.0053\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.7569 - val_loss: 0.6363\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6177 - val_loss: 0.5953\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5963 - val_loss: 0.5804\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5860 - val_loss: 0.5705\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.5771 - val_loss: 0.5614\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5702 - val_loss: 0.5548\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5640 - val_loss: 0.5481\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5589 - val_loss: 0.5427\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5544 - val_loss: 0.5389\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5511 - val_loss: 0.5347\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5479 - val_loss: 0.5313\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5455 - val_loss: 0.5290\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5431 - val_loss: 0.5261\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5416 - val_loss: 0.5252\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5396 - val_loss: 0.5225\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5384 - val_loss: 0.5229\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5373 - val_loss: 0.5194\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5367 - val_loss: 0.5197\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5352 - val_loss: 0.5212\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5354 - val_loss: 0.5176\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5345 - val_loss: 0.5173\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5338 - val_loss: 0.5161\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5337 - val_loss: 0.5166\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5331 - val_loss: 0.5146\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5332 - val_loss: 0.5158\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5329 - val_loss: 0.5161\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5325 - val_loss: 0.5157\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5326 - val_loss: 0.5152\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5321 - val_loss: 0.5164\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 0.5395\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 2s 227us/sample - loss: 1.6024 - val_loss: 0.9029\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.7890 - val_loss: 0.7517\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6893 - val_loss: 0.6854\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6401 - val_loss: 0.6440\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.6041 - val_loss: 0.6080\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5761 - val_loss: 0.5816\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5551 - val_loss: 0.5577\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5374 - val_loss: 0.5398\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5230 - val_loss: 0.5244\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5108 - val_loss: 0.5123\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5010 - val_loss: 0.5020\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4921 - val_loss: 0.4931\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4850 - val_loss: 0.4854\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4783 - val_loss: 0.4780\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4729 - val_loss: 0.4717\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4675 - val_loss: 0.4669\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4631 - val_loss: 0.4619\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4590 - val_loss: 0.4578\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4549 - val_loss: 0.4530\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4513 - val_loss: 0.4502\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4486 - val_loss: 0.4459\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4453 - val_loss: 0.4435\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4428 - val_loss: 0.4401\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4398 - val_loss: 0.4383\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4369 - val_loss: 0.4353\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4353 - val_loss: 0.4332\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4326 - val_loss: 0.4303\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4307 - val_loss: 0.4280\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4285 - val_loss: 0.4266\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4265 - val_loss: 0.4237\n",
            "3870/3870 [==============================] - 0s 29us/sample - loss: 0.4378\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 2s 238us/sample - loss: 1.9579 - val_loss: 0.8383\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.7635 - val_loss: 0.7151\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6792 - val_loss: 0.6637\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6388 - val_loss: 0.6265\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.6052 - val_loss: 0.5950\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5829 - val_loss: 0.5756\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5603 - val_loss: 0.5526\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5424 - val_loss: 0.5354\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5289 - val_loss: 0.5232\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5166 - val_loss: 0.5098\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5069 - val_loss: 0.5010\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4976 - val_loss: 0.4931\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4906 - val_loss: 0.4858\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4837 - val_loss: 0.4811\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4784 - val_loss: 0.4738\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4735 - val_loss: 0.4701\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4695 - val_loss: 0.4666\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4650 - val_loss: 0.4614\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4620 - val_loss: 0.4578\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4575 - val_loss: 0.4547\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4551 - val_loss: 0.4517\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4512 - val_loss: 0.4497\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4487 - val_loss: 0.4459\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4460 - val_loss: 0.4444\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4427 - val_loss: 0.4430\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4412 - val_loss: 0.4395\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4387 - val_loss: 0.4357\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4376 - val_loss: 0.4337\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4333 - val_loss: 0.4350\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4333 - val_loss: 0.4315\n",
            "3870/3870 [==============================] - 0s 29us/sample - loss: 0.4523\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 2s 243us/sample - loss: 1.8761 - val_loss: 0.9361\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.8090 - val_loss: 0.7672\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.7242 - val_loss: 0.7061\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6782 - val_loss: 0.6652\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6433 - val_loss: 0.6296\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.6146 - val_loss: 0.6021\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5919 - val_loss: 0.5783\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5721 - val_loss: 0.5595\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5555 - val_loss: 0.5415\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5418 - val_loss: 0.5263\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 1s 69us/sample - loss: 0.5294 - val_loss: 0.5145\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 1s 67us/sample - loss: 0.5190 - val_loss: 0.5048\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.5101 - val_loss: 0.4952\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 1s 68us/sample - loss: 0.5021 - val_loss: 0.4876\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4949 - val_loss: 0.4808\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4891 - val_loss: 0.4734\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4836 - val_loss: 0.4674\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4781 - val_loss: 0.4640\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4736 - val_loss: 0.4588\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4691 - val_loss: 0.4549\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4650 - val_loss: 0.4519\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4618 - val_loss: 0.4476\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4587 - val_loss: 0.4444\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4556 - val_loss: 0.4414\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4526 - val_loss: 0.4384\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4502 - val_loss: 0.4358\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4477 - val_loss: 0.4339\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4449 - val_loss: 0.4334\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4429 - val_loss: 0.4296\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4405 - val_loss: 0.4271\n",
            "3870/3870 [==============================] - 0s 30us/sample - loss: 0.4262\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 2s 232us/sample - loss: 1.1602 - val_loss: 0.5295\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5857 - val_loss: 0.5107\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6504 - val_loss: 0.5260\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.7257 - val_loss: 0.5166\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 1.2524 - val_loss: 0.7470\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 1.4056 - val_loss: 0.5154\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 1.2803 - val_loss: 1.2518\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 1.2069 - val_loss: 0.5104\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 1.3008 - val_loss: 0.4955\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 2.2026 - val_loss: 0.4821\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 1.6800 - val_loss: 0.4959\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 3.1929 - val_loss: 0.9491\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 2.7190 - val_loss: 0.4838\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 4.2756 - val_loss: 0.5630\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 8.3526 - val_loss: 0.5004\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 6.0705 - val_loss: 0.4834\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 3.9005 - val_loss: 0.5641\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 6.1563 - val_loss: 2.4705\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 41.0970 - val_loss: 1.7229\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 43.0344 - val_loss: 2.4398\n",
            "3870/3870 [==============================] - 0s 28us/sample - loss: 359.4781\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 2s 248us/sample - loss: 0.9283 - val_loss: 0.6586\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 3.1881 - val_loss: 1.3515\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 231.0170 - val_loss: 109.3686\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 17895.9394 - val_loss: 9976.6542\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 1595418.8319 - val_loss: 700301.4205\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 225599503.6999 - val_loss: 58006617.8388\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 17583563772.1633 - val_loss: 4805775433.4264\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 1566504450482.4724 - val_loss: 403370907769.7158\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 118613362111935.1719 - val_loss: 46781416811466.5547\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 6049330149916909.0000 - val_loss: 3387684084505874.5000\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 880747875702712064.0000 - val_loss: 229186366091821216.0000\n",
            "3870/3870 [==============================] - 0s 24us/sample - loss: 34491998557984284672.0000\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 2s 240us/sample - loss: 9.7102 - val_loss: 0.5936\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 558.8013 - val_loss: 19.0988\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 35578.9337 - val_loss: 1681.8774\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 19113004.4848 - val_loss: 318916.6593\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 1233860252.3762 - val_loss: 28664380.6083\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 78935011549.6475 - val_loss: 3847787942.7638\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 1171100468707.8201 - val_loss: 888049472048.9509\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 3481588120995070.5000 - val_loss: 788914424057587.1250\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 28048885155707616.0000 - val_loss: 8190323386840747.0000\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 48834811192759205888.0000 - val_loss: 1095410616834872576.0000\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 348323992589130268672.0000 - val_loss: 144110753351682473984.0000\n",
            "3870/3870 [==============================] - 0s 26us/sample - loss: 323508996190754373632.0000\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 2s 262us/sample - loss: 1.2892 - val_loss: 0.6692\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6108 - val_loss: 0.5862\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5549 - val_loss: 0.5392\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5215 - val_loss: 0.5121\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4972 - val_loss: 0.4906\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4807 - val_loss: 0.4714\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4652 - val_loss: 0.4568\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4548 - val_loss: 0.4455\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4459 - val_loss: 0.4348\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4374 - val_loss: 0.4259\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4320 - val_loss: 0.4244\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4269 - val_loss: 0.4171\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4225 - val_loss: 0.4172\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4191 - val_loss: 0.4099\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4156 - val_loss: 0.4070\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4138 - val_loss: 0.4050\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4117 - val_loss: 0.4012\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4075 - val_loss: 0.4003\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4039 - val_loss: 0.4025\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4012 - val_loss: 0.3949\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3988 - val_loss: 0.3923\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3951 - val_loss: 0.3905\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3938 - val_loss: 0.3869\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3896 - val_loss: 0.3873\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3880 - val_loss: 0.3882\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3952 - val_loss: 0.3848\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3839 - val_loss: 0.3802\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3806 - val_loss: 0.3842\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3813 - val_loss: 0.3762\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3779 - val_loss: 0.3772\n",
            "3870/3870 [==============================] - 0s 25us/sample - loss: 0.3822\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 2s 262us/sample - loss: 1.7122 - val_loss: 0.6892\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 1.2651 - val_loss: 0.6584\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6111 - val_loss: 0.5812\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5621 - val_loss: 0.5375\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5265 - val_loss: 0.5074\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4990 - val_loss: 0.4826\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4783 - val_loss: 0.4616\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4641 - val_loss: 0.4483\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4535 - val_loss: 0.4388\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4459 - val_loss: 0.4311\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4388 - val_loss: 0.4272\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4323 - val_loss: 0.4232\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4282 - val_loss: 0.4190\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4254 - val_loss: 0.4126\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4191 - val_loss: 0.4099\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4155 - val_loss: 0.4064\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4122 - val_loss: 0.4019\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4082 - val_loss: 0.4002\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4050 - val_loss: 0.3953\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4022 - val_loss: 0.3930\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3987 - val_loss: 0.3897\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3949 - val_loss: 0.3910\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3921 - val_loss: 0.3850\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3887 - val_loss: 0.3835\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3848 - val_loss: 0.3794\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3820 - val_loss: 0.3797\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3794 - val_loss: 0.3809\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3765 - val_loss: 0.3750\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3742 - val_loss: 0.3731\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3716 - val_loss: 0.3698\n",
            "3870/3870 [==============================] - 0s 30us/sample - loss: 0.3903\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 2s 269us/sample - loss: 1.2890 - val_loss: 0.7003\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6571 - val_loss: 0.6304\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6036 - val_loss: 0.5843\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5653 - val_loss: 0.5431\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5352 - val_loss: 0.5133\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5107 - val_loss: 0.4904\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4933 - val_loss: 0.4777\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4807 - val_loss: 0.4636\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4699 - val_loss: 0.4567\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4627 - val_loss: 0.4472\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4547 - val_loss: 0.4471\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4487 - val_loss: 0.4374\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4443 - val_loss: 0.4312\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4390 - val_loss: 0.4258\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4347 - val_loss: 0.4224\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4326 - val_loss: 0.4181\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4284 - val_loss: 0.4205\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4243 - val_loss: 0.4120\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4214 - val_loss: 0.4095\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4186 - val_loss: 0.4059\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4138 - val_loss: 0.4044\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4126 - val_loss: 0.4001\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4095 - val_loss: 0.4009\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4074 - val_loss: 0.4018\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4045 - val_loss: 0.3956\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4025 - val_loss: 0.3934\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4008 - val_loss: 0.3911\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3989 - val_loss: 0.3888\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3951 - val_loss: 0.3882\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3959 - val_loss: 0.3828\n",
            "3870/3870 [==============================] - 0s 29us/sample - loss: 0.3850\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 2s 271us/sample - loss: 1.5476 - val_loss: 0.8475\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.7395 - val_loss: 0.7313\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6825 - val_loss: 0.6835\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6492 - val_loss: 0.6519\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6222 - val_loss: 0.6236\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5990 - val_loss: 0.6007\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5799 - val_loss: 0.5802\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5635 - val_loss: 0.5627\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5494 - val_loss: 0.5481\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5370 - val_loss: 0.5353\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5257 - val_loss: 0.5246\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5175 - val_loss: 0.5149\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5087 - val_loss: 0.5053\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5009 - val_loss: 0.4979\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4954 - val_loss: 0.4910\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4895 - val_loss: 0.4857\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4837 - val_loss: 0.4806\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4804 - val_loss: 0.4765\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4759 - val_loss: 0.4711\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4717 - val_loss: 0.4672\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4683 - val_loss: 0.4648\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4654 - val_loss: 0.4603\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4628 - val_loss: 0.4573\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4601 - val_loss: 0.4560\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4578 - val_loss: 0.4539\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4558 - val_loss: 0.4516\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4533 - val_loss: 0.4485\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4518 - val_loss: 0.4464\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4502 - val_loss: 0.4443\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4476 - val_loss: 0.4446\n",
            "3870/3870 [==============================] - 0s 26us/sample - loss: 0.4642\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 2s 275us/sample - loss: 1.9827 - val_loss: 0.8915\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.7882 - val_loss: 0.7295\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.6950 - val_loss: 0.6740\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6576 - val_loss: 0.6442\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6279 - val_loss: 0.6175\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.6090 - val_loss: 0.5976\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5895 - val_loss: 0.5807\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5745 - val_loss: 0.5644\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5629 - val_loss: 0.5528\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5515 - val_loss: 0.5413\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5412 - val_loss: 0.5310\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5318 - val_loss: 0.5217\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5239 - val_loss: 0.5135\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5142 - val_loss: 0.5059\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5089 - val_loss: 0.4996\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5026 - val_loss: 0.4931\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4980 - val_loss: 0.4885\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4927 - val_loss: 0.4844\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4876 - val_loss: 0.4811\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4845 - val_loss: 0.4766\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4800 - val_loss: 0.4735\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4777 - val_loss: 0.4704\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4736 - val_loss: 0.4670\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4712 - val_loss: 0.4644\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4675 - val_loss: 0.4623\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4649 - val_loss: 0.4586\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4616 - val_loss: 0.4568\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4597 - val_loss: 0.4536\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4569 - val_loss: 0.4515\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4547 - val_loss: 0.4490\n",
            "3870/3870 [==============================] - 0s 30us/sample - loss: 0.4650\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "7740/7740 [==============================] - 2s 287us/sample - loss: 2.1166 - val_loss: 0.8583\n",
            "Epoch 2/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.7325 - val_loss: 0.6966\n",
            "Epoch 3/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.6681 - val_loss: 0.6583\n",
            "Epoch 4/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6365 - val_loss: 0.6278\n",
            "Epoch 5/30\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6113 - val_loss: 0.6011\n",
            "Epoch 6/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5892 - val_loss: 0.5786\n",
            "Epoch 7/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5706 - val_loss: 0.5611\n",
            "Epoch 8/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5550 - val_loss: 0.5433\n",
            "Epoch 9/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5414 - val_loss: 0.5301\n",
            "Epoch 10/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5305 - val_loss: 0.5173\n",
            "Epoch 11/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5212 - val_loss: 0.5066\n",
            "Epoch 12/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5133 - val_loss: 0.4996\n",
            "Epoch 13/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5063 - val_loss: 0.4917\n",
            "Epoch 14/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5005 - val_loss: 0.4865\n",
            "Epoch 15/30\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4953 - val_loss: 0.4812\n",
            "Epoch 16/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4914 - val_loss: 0.4758\n",
            "Epoch 17/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4874 - val_loss: 0.4716\n",
            "Epoch 18/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4843 - val_loss: 0.4680\n",
            "Epoch 19/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4812 - val_loss: 0.4648\n",
            "Epoch 20/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4781 - val_loss: 0.4626\n",
            "Epoch 21/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4757 - val_loss: 0.4601\n",
            "Epoch 22/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4733 - val_loss: 0.4574\n",
            "Epoch 23/30\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4714 - val_loss: 0.4548\n",
            "Epoch 24/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4691 - val_loss: 0.4524\n",
            "Epoch 25/30\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4668 - val_loss: 0.4505\n",
            "Epoch 26/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4648 - val_loss: 0.4483\n",
            "Epoch 27/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4632 - val_loss: 0.4468\n",
            "Epoch 28/30\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4615 - val_loss: 0.4442\n",
            "Epoch 29/30\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4593 - val_loss: 0.4442\n",
            "Epoch 30/30\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4583 - val_loss: 0.4427\n",
            "3870/3870 [==============================] - 0s 29us/sample - loss: 0.4356\n",
            "Train on 11610 samples, validate on 3870 samples\n",
            "Epoch 1/30\n",
            "11610/11610 [==============================] - 2s 209us/sample - loss: 1.1214 - val_loss: 0.6322\n",
            "Epoch 2/30\n",
            "11610/11610 [==============================] - 1s 54us/sample - loss: 0.5859 - val_loss: 0.5511\n",
            "Epoch 3/30\n",
            "11610/11610 [==============================] - 1s 55us/sample - loss: 0.5362 - val_loss: 0.5032\n",
            "Epoch 4/30\n",
            "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4957 - val_loss: 0.4712\n",
            "Epoch 5/30\n",
            "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4779 - val_loss: 0.4530\n",
            "Epoch 6/30\n",
            "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4666 - val_loss: 0.4435\n",
            "Epoch 7/30\n",
            "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4493 - val_loss: 0.4301\n",
            "Epoch 8/30\n",
            "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4405 - val_loss: 0.4196\n",
            "Epoch 9/30\n",
            "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4311 - val_loss: 0.4145\n",
            "Epoch 10/30\n",
            "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4260 - val_loss: 0.4105\n",
            "Epoch 11/30\n",
            "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4210 - val_loss: 0.4088\n",
            "Epoch 12/30\n",
            "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4212 - val_loss: 0.4029\n",
            "Epoch 13/30\n",
            "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4145 - val_loss: 0.3994\n",
            "Epoch 14/30\n",
            "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4451 - val_loss: 0.3967\n",
            "Epoch 15/30\n",
            "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4128 - val_loss: 0.3925\n",
            "Epoch 16/30\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.4025 - val_loss: 0.3905\n",
            "Epoch 17/30\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3986 - val_loss: 0.3854\n",
            "Epoch 18/30\n",
            "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3960 - val_loss: 0.3821\n",
            "Epoch 19/30\n",
            "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3924 - val_loss: 0.3809\n",
            "Epoch 20/30\n",
            "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3896 - val_loss: 0.3801\n",
            "Epoch 21/30\n",
            "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3868 - val_loss: 0.3747\n",
            "Epoch 22/30\n",
            "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3844 - val_loss: 0.3743\n",
            "Epoch 23/30\n",
            "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3830 - val_loss: 0.3711\n",
            "Epoch 24/30\n",
            "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3808 - val_loss: 0.3678\n",
            "Epoch 25/30\n",
            "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3775 - val_loss: 0.3676\n",
            "Epoch 26/30\n",
            "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3748 - val_loss: 0.3695\n",
            "Epoch 27/30\n",
            "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3734 - val_loss: 0.3653\n",
            "Epoch 28/30\n",
            "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3720 - val_loss: 0.3604\n",
            "Epoch 29/30\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3687 - val_loss: 0.3602\n",
            "Epoch 30/30\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3662 - val_loss: 0.3593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
              "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7eff4f9085f8>,\n",
              "                   iid='warn', n_iter=10, n_jobs=None,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7eff4f380c18>,\n",
              "                                        'n_hidden': [0, 1, 2, 3],\n",
              "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10,...\n",
              "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
              "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
              "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
              "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
              "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])},\n",
              "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNTvtpHIOPzb",
        "colab_type": "text"
      },
      "source": [
        "When it is over, you can access the best parameters found, the best score, and the trained Keras model like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWzCxpdJL8dH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cdc12109-4249-49a0-de2b-b0bb37c4b836"
      },
      "source": [
        "rnd_search_cv.best_params_"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.004835748729462287, 'n_hidden': 2, 'n_neurons': 24}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4IMnJ40OgGm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cf3cbaa-e469-4ff6-c9da-b1554cfc8499"
      },
      "source": [
        "rnd_search_cv.best_score_"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.3858444584252812"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAUybHjzOjeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = rnd_search_cv.best_estimator_.model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFZhBzpROlXU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2cdc452-01be-486a-a3ec-f2e66af24f37"
      },
      "source": [
        "model"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.sequential.Sequential at 0x7eff4bed1f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEFkEBlwOmZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}